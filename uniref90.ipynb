{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecc84ac-56b8-4eb7-8c89-1f8c75e7b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datasets import load_dataset, load_from_disk\n",
    "import pyarrow.parquet as pq  \n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\"\n",
    "table = pq.read_table(parquet_path)\n",
    "clusters = table.to_pylist()\n",
    "print(f\"Loaded {len(clusters)} clusters into memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a67daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import random\n",
    "\n",
    "class CachedRowGroupClusterSampler:\n",
    "    def __init__(self, parquet_path):\n",
    "        self.pf = pq.ParquetFile(parquet_path)\n",
    "        self.num_row_groups = self.pf.num_row_groups\n",
    "        self.row_group_cache = []\n",
    "        self.current_rg_index = None\n",
    "\n",
    "    def sample_cluster(self):\n",
    "        # If cache is empty, load a new row group\n",
    "        if not self.row_group_cache:\n",
    "            # Choose a random row group\n",
    "            rg_idx = random.randint(0, self.num_row_groups - 1)\n",
    "            table = self.pf.read_row_group(rg_idx)\n",
    "            rows = table.to_pylist()  # 1840 rows typically\n",
    "            random.shuffle(rows)  # Shuffle to maintain randomness\n",
    "            self.row_group_cache.extend(rows)\n",
    "            self.current_rg_index = rg_idx\n",
    "\n",
    "        # Pop one sample from cache\n",
    "        row = self.row_group_cache.pop()\n",
    "        return {\n",
    "            \"cluster_id\": row[\"cluster_id\"],\n",
    "            \"representative_id\": row[\"representative_id\"],\n",
    "            \"members\": row[\"members\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomClusterSampler:\n",
    "    def __init__(self, parquet_path):\n",
    "        self.path = parquet_path\n",
    "        \n",
    "        # Read metadata once\n",
    "        self.meta = pq.read_metadata(parquet_path)\n",
    "        self.num_row_groups = self.meta.num_row_groups \n",
    "        \n",
    "        # Open ParquetFile once \n",
    "        self.pf = pq.ParquetFile(parquet_path)\n",
    "\n",
    "    def sample_clusters(self):\n",
    "        # randomly select a row group (multiple clusters per row group)\n",
    "        rg = np.random.randint(0, self.num_row_groups)\n",
    "\n",
    "        # read only that row group \n",
    "        table = self.pf.read_row_group(rg)\n",
    "        n = table.num_rows\n",
    "\n",
    "        # Pick random row (cluster) from the row group\n",
    "        cluster_idx = np.random.randint(0, n)\n",
    "        row = table.slice(cluster_idx, 1)\n",
    "\n",
    "        cluster_id = row[\"cluster_id\"][0].as_py()\n",
    "        rep_id      = row[\"representative_id\"][0].as_py()\n",
    "        members     = row[\"members\"][0].as_py()\n",
    "\n",
    "        return {\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"representative_id\": rep_id,\n",
    "            \"members\": members\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryClusterSampler:\n",
    "    def __init__(self, parquet_path):\n",
    "        print(f\"Loading Parquet file into memory (Arrow table): {parquet_path}...\")\n",
    "        self.table = pq.read_table(parquet_path)  \n",
    "        self.num_rows = self.table.num_rows\n",
    "        print(f\"Loaded Arrow table with {self.num_rows} clusters.\")\n",
    "\n",
    "    def sample_cluster(self):\n",
    "        idx = random.randint(0, self.num_rows - 1)\n",
    "        row = {k: self.table[k][idx].as_py() for k in self.table.column_names}\n",
    "        return {\n",
    "            \"cluster_id\": row[\"cluster_id\"],\n",
    "            \"representative_id\": row[\"representative_id\"],\n",
    "            \"members\": row[\"members\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab977b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "Sampler1 Time taken: 0.8135685920715332 seconds\n"
     ]
    }
   ],
   "source": [
    "parquet_path = \"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\"\n",
    "sampeler1_start_time = time.time()\n",
    "sampler1 = CachedRowGroupClusterSampler(parquet_path)\n",
    "# Example usage\n",
    "sampler1_samples = []\n",
    "for i in range(129):\n",
    "    sample = sampler1.sample_cluster()\n",
    "    sampler1_samples.append(sample)\n",
    "print(len(sampler1_samples))  # Print a sampled cluster\n",
    "sampeler1_end_time = time.time()\n",
    "print(f\"Sampler1 Time taken: {sampeler1_end_time - sampeler1_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af74bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "Sampler1 Time taken: 5.8889000415802 seconds\n"
     ]
    }
   ],
   "source": [
    "parquet_path = \"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\"\n",
    "sampeler2_start_time = time.time()\n",
    "sampler2 = RandomClusterSampler(parquet_path)\n",
    "# Example usage\n",
    "sampler2_samples = []\n",
    "for i in range(129):\n",
    "    sample = sampler2.sample_clusters()\n",
    "    sampler2_samples.append(sample)\n",
    "print(len(sampler2_samples))  # Print a sampled cluster\n",
    "sampeler2_end_time = time.time()\n",
    "print(f\"Sampler1 Time taken: {sampeler2_end_time - sampeler2_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab6f73-5984-4c19-b34a-4990c37164a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 184146434 examples [00:20, 8911737.66 examples/s] \n"
     ]
    }
   ],
   "source": [
    "meta = load_dataset(\"parquet\", \n",
    "                    data_files=\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63674a25-bd53-457c-b275-885b049c972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b507972-76a7-4b36-b243-ce2e4d11c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce55da9-92fe-4a8e-8685-f63221da97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 184146434 examples [00:18, 9966633.86 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_parquet(\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185168c-0386-4c52-84cf-8c9f299d886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['cluster_id', 'representative_id', 'member_count_xml_file', 'member_count', 'members'],\n",
      "    num_rows: 184146434\n",
      "})\n",
      "{'cluster_id': Value(dtype='string', id=None), 'representative_id': Value(dtype='string', id=None), 'member_count_xml_file': Value(dtype='int32', id=None), 'member_count': Value(dtype='int32', id=None), 'members': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files=\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\",\n",
    "    split=\"train\",      # required if not streaming\n",
    "    streaming=False     # or just omit this arg\n",
    ")\n",
    "\n",
    "print(ds)\n",
    "print(ds.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572d5cb-52fa-4839-85e9-327c723153fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['cluster_id', 'representative_id', 'member_count_xml_file', 'member_count', 'members'],\n",
      "    num_rows: 184146434\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from datasets import Dataset\n",
    "\n",
    "table = pq.read_table(\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\")\n",
    "ds = Dataset(table)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd81f91-a774-424f-9131-34b8e3fdc76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "cluster_id: string\n",
       "representative_id: string\n",
       "member_count_xml_file: int32\n",
       "member_count: int32\n",
       "members: list<element: string>\n",
       "  child 0, element: string\n",
       "----\n",
       "cluster_id: [[\"UniRef90_UPI002E2621C6\",\"UniRef90_UPI00358F51CD\",\"UniRef90_UPI00398E31D8\",\"UniRef90_A0A5A9P0L4\",\"UniRef90_A0ABD1JBH0\",...,\"UniRef90_UPI00225B580A\",\"UniRef90_UPI00254460F6\",\"UniRef90_UPI002796F535\",\"UniRef90_UPI002896634E\",\"UniRef90_UPI003D7BF59D\"],[\"UniRef90_UPI00222E980F\",\"UniRef90_UPI0024785306\",\"UniRef90_UPI0035A280ED\",\"UniRef90_UPI001B351495\",\"UniRef90_UPI00222EC2CE\",...,\"UniRef90_UPI00248289A7\",\"UniRef90_UPI0012FEA33D\",\"UniRef90_A0A8C5NYK4\",\"UniRef90_UPI0025ADAEBD\",\"UniRef90_UPI003BF9D085\"],...,[\"UniRef90_A0A3M7PZG0\",\"UniRef90_Q9BM67\",\"UniRef90_Q9BM66\",\"UniRef90_E9BBM2\",\"UniRef90_Q9BLZ4\",...,\"UniRef90_A0A0T5Z569\",\"UniRef90_D4J9Q7\",\"UniRef90_A0A6A7YBE0\",\"UniRef90_A0A0T5Z260\",\"UniRef90_A0A5C6IVY6\"],[\"UniRef90_Q725V0\",\"UniRef90_A0A4R4SIP9\",\"UniRef90_A0ABW1B1Z7\",\"UniRef90_A0A943LZD8\",\"UniRef90_A0ABV6HH89\",...,\"UniRef90_W8CWU6\",\"UniRef90_A0A1S5R4P5\",\"UniRef90_W8CWV6\",\"UniRef90_F1AU43\",\"UniRef90_K0JAJ8\"]]\n",
       "representative_id: [[\"UPI002E2621C6\",\"UPI00358F51CD\",\"UPI00398E31D8\",\"A0A5A9P0L4_9TELE\",\"A0ABD1JBH0_9TELE\",...,\"UPI00225B580A\",\"UPI00254460F6\",\"UPI002796F535\",\"UPI002896634E\",\"UPI003D7BF59D\"],[\"UPI00222E980F\",\"UPI0024785306\",\"UPI0035A280ED\",\"UPI001B351495\",\"UPI00222EC2CE\",...,\"UPI00248289A7\",\"UPI0012FEA33D\",\"A0A8C5NYK4_JACJA\",\"UPI0025ADAEBD\",\"UPI003BF9D085\"],...,[\"A0A3M7PZG0_BRAPC\",\"Q9BM67_BRAPC\",\"Q9BM66_BRAPC\",\"E9BBM2_LEIDO\",\"Q9BLZ4_ADIVA\",...,\"A0A0T5Z569_9GAMM\",\"D4J9Q7_9FIRM\",\"A0A6A7YBE0_9HYPH\",\"A0A0T5Z260_9GAMM\",\"A0A5C6IVY6_9ACTN\"],[\"Q725V0_NITV2\",\"A0A4R4SIP9_9ACTN\",\"A0ABW1B1Z7_9ACTN\",\"A0A943LZD8_STRVE\",\"A0ABV6HH89_9SPHI\",...,\"W8CWU6_HV1\",\"A0A1S5R4P5_HV1\",\"W8CWV6_HV1\",\"F1AU43_9RETR\",\"K0JAJ8_HV1\"]]\n",
       "member_count_xml_file: [[1,1,1,1,1,...,1,1,1,3,17],[1,2,1,24,1,...,1,1,5,1,17],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]]\n",
       "member_count: [[1,1,1,1,1,...,1,1,1,3,17],[1,2,1,24,1,...,1,1,5,1,17],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]]\n",
       "members: [[[],[],...,[\"UPI001E2D841F\",\"UPI00117FD49D\"],[\"UPI003D7B8CDB\",\"UPI003D7978EC\",\"UPI003D7AC66D\",\"UPI003D792C38\",\"UPI003D7A1F94\",...,\"UPI003D78C3B4\",\"UPI003D7A3140\",\"UPI003D794833\",\"UPI003D7B55EB\",\"UPI003D78CC4B\"]],[[],[\"UPI00247AFB57\"],...,[],[\"UPI0028C4FAB0\",\"UPI002FC3901D\",\"UPI003B27D44B\",\"UPI0028C3F07B\",\"UPI003BFA1CEA\",...,\"UPI0028C494C7\",\"UPI002FC2F640\",\"UPI003B27EEB1\",\"UPI003B2892C2\",\"UPI003B2830F9\"]],...,[[],[],...,[],[]],[[],[],...,[],[]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pq.read_table(\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\")\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e635b-0053-4ec7-8478-6dfc0ee9ff69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184146434"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.read_metadata(\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\").num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e86bba-084d-4e02-b790-0263da0f4661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_id: string\n",
       "representative_id: string\n",
       "member_count_xml_file: int32\n",
       "member_count: int32\n",
       "members: list<element: string>\n",
       "  child 0, element: string"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef186a-52e4-4263-92f3-a500e1241348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClusterID</th>\n",
       "      <th>RepresentativeID</th>\n",
       "      <th>member_count_xml_file</th>\n",
       "      <th>member_count</th>\n",
       "      <th>MemberIDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UniRef90_UPI002E2621C6</td>\n",
       "      <td>UPI002E2621C6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UniRef90_UPI00358F51CD</td>\n",
       "      <td>UPI00358F51CD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UniRef90_UPI00398E31D8</td>\n",
       "      <td>UPI00398E31D8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UniRef90_A0A5A9P0L4</td>\n",
       "      <td>A0A5A9P0L4_9TELE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UniRef90_A0ABD1JBH0</td>\n",
       "      <td>A0ABD1JBH0_9TELE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>UniRef90_A0A6P8RG45</td>\n",
       "      <td>A0A6P8RG45_GEOSA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>UniRef90_UPI003D69693D</td>\n",
       "      <td>UPI003D69693D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>UniRef90_UPI003EBDD6D2</td>\n",
       "      <td>UPI003EBDD6D2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>UPI003EBC14C6,UPI003EB9A168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>UniRef90_A0A1V4K6M4</td>\n",
       "      <td>A0A1V4K6M4_PATFA</td>\n",
       "      <td>562</td>\n",
       "      <td>562</td>\n",
       "      <td>A0A7K4RU14_COLPI,A0A7L4G2H3_9COLU,A0A094KAD8_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>UniRef90_UPI003AB6E641</td>\n",
       "      <td>UPI003AB6E641</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ClusterID  RepresentativeID  member_count_xml_file  \\\n",
       "0   UniRef90_UPI002E2621C6     UPI002E2621C6                      1   \n",
       "1   UniRef90_UPI00358F51CD     UPI00358F51CD                      1   \n",
       "2   UniRef90_UPI00398E31D8     UPI00398E31D8                      1   \n",
       "3      UniRef90_A0A5A9P0L4  A0A5A9P0L4_9TELE                      1   \n",
       "4      UniRef90_A0ABD1JBH0  A0ABD1JBH0_9TELE                      1   \n",
       "..                     ...               ...                    ...   \n",
       "95     UniRef90_A0A6P8RG45  A0A6P8RG45_GEOSA                      1   \n",
       "96  UniRef90_UPI003D69693D     UPI003D69693D                      1   \n",
       "97  UniRef90_UPI003EBDD6D2     UPI003EBDD6D2                      3   \n",
       "98     UniRef90_A0A1V4K6M4  A0A1V4K6M4_PATFA                    562   \n",
       "99  UniRef90_UPI003AB6E641     UPI003AB6E641                      1   \n",
       "\n",
       "    member_count                                          MemberIDs  \n",
       "0              1                                                NaN  \n",
       "1              1                                                NaN  \n",
       "2              1                                                NaN  \n",
       "3              1                                                NaN  \n",
       "4              1                                                NaN  \n",
       "..           ...                                                ...  \n",
       "95             1                                                NaN  \n",
       "96             1                                                NaN  \n",
       "97             3                        UPI003EBC14C6,UPI003EB9A168  \n",
       "98           562  A0A7K4RU14_COLPI,A0A7L4G2H3_9COLU,A0A094KAD8_A...  \n",
       "99             1                                                NaN  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/gpfs/data/brandeslab/Data/uniref/uniref90_cluster_members.tsv\", sep=\"\\t\", nrows=100)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0563e10-8702-49e4-bd7e-a4aa1bffc1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClusterID                     UniRef90_UPI003EBDD6D2\n",
       "RepresentativeID                       UPI003EBDD6D2\n",
       "member_count_xml_file                              3\n",
       "member_count                                       3\n",
       "MemberIDs                UPI003EBC14C6,UPI003EB9A168\n",
       "Name: 97, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6dbd03-90da-49d6-9891-d64afeb33660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parasail in /gpfs/data/brandeslab/User/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy in /gpfs/data/brandeslab/User/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages (from parasail) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install parasail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4c668-6305-4436-b441-99be75104e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import parasail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c08ffe-a83c-43c4-acc0-192d8dff2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parasail in /gpfs/data/brandeslab/User/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy in /gpfs/data/brandeslab/User/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages (from parasail) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install parasail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbb846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, '-': 5, 'A': 6, 'C': 7, 'D': 8, 'E': 9, 'F': 10, 'G': 11, 'H': 12, 'I': 13, 'K': 14, 'L': 15, 'M': 16, 'N': 17, 'P': 18, 'Q': 19, 'R': 20, 'S': 21, 'T': 22, 'V': 23, 'W': 24, 'Y': 25}\n",
      "'-' ID: 5\n",
      "\n",
      "Tests:\n",
      "'-' converts to: 5\n",
      "Tokenization of 'ACD-GH': {'input_ids': [6, 7, 8, 5, 11, 12], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "Decoding: A C D - G H\n",
      "Vocab size: 26\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Split\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Define vocabulary: ONLY standard 20 amino acids\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")  # Standard 20 only\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"-\"]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = {token: i for i, token in enumerate(special_tokens + amino_acids)}\n",
    "\n",
    "print(f\"Vocab: {vocab}\")\n",
    "print(f\"'-' ID: {vocab['-']}\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Split(pattern=\"\", behavior=\"isolated\")\n",
    "\n",
    "# Wrap it\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "\n",
    "# Test\n",
    "print(\"\\nTests:\")\n",
    "print(\"'-' converts to:\", hf_tokenizer.convert_tokens_to_ids(\"-\"))\n",
    "print(\"Tokenization of 'ACD-GH':\", hf_tokenizer(\"ACD-GH\"))\n",
    "print(\"Decoding:\", hf_tokenizer.decode([6, 7, 8, 5, 11, 12]))\n",
    "print(\"Vocab size:\", len(hf_tokenizer))\n",
    "\n",
    "# hf_tokenizer.save_pretrained(\"phylo_char_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a22595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, '[GAP]': 5, '-': 6, 'A': 7, 'C': 8, 'D': 9, 'E': 10, 'F': 11, 'G': 12, 'H': 13, 'I': 14, 'K': 15, 'L': 16, 'M': 17, 'N': 18, 'P': 19, 'Q': 20, 'R': 21, 'S': 22, 'T': 23, 'V': 24, 'W': 25, 'Y': 26}\n",
      "'-' ID: 6\n",
      "[GAP] ID: 5\n",
      "[MASK] ID: 4\n",
      "\n",
      "Tests:\n",
      "'-' converts to: 6\n",
      "[GAP] converts to: 5\n",
      "[MASK] converts to: 4\n",
      "\n",
      "\n",
      "Tokenization of 'ACD-GH': {'input_ids': [7, 8, 9, 6, 12, 13], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "Tokenization of  'ACD[GAP]GH': {'input_ids': [7, 8, 9, 5, 12, 13], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "Vocab size: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('phylo_char_tokenizer_updated/tokenizer_config.json',\n",
       " 'phylo_char_tokenizer_updated/special_tokens_map.json',\n",
       " 'phylo_char_tokenizer_updated/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Split\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Define vocabulary: ONLY standard 20 amino acids\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")  # Standard 20 only\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[GAP]\", \"-\"]\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = {token: i for i, token in enumerate(special_tokens + amino_acids)}\n",
    "\n",
    "print(f\"Vocab: {vocab}\")\n",
    "print(f\"'-' ID: {vocab['-']}\")\n",
    "print(f\"[GAP] ID: {vocab['[GAP]']}\")\n",
    "print(f\"[MASK] ID: {vocab['[MASK]']}\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Split(pattern=\"\", behavior=\"isolated\")\n",
    "\n",
    "# Wrap it\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    additional_special_tokens=[\"[GAP]\"]  # ðŸ‘ˆ Register [GAP] as special\n",
    ")\n",
    "\n",
    "# Test\n",
    "print(\"\\nTests:\")\n",
    "print(\"'-' converts to:\", hf_tokenizer.convert_tokens_to_ids(\"-\"))\n",
    "print(\"[GAP] converts to:\", hf_tokenizer.convert_tokens_to_ids(\"[GAP]\"))\n",
    "print(\"[MASK] converts to:\", hf_tokenizer.convert_tokens_to_ids(\"[MASK]\"))\n",
    "print(\"\\n\")\n",
    "print(\"Tokenization of 'ACD-GH':\", hf_tokenizer(\"ACD-GH\", add_special_tokens=False))\n",
    "print(\"Tokenization of  'ACD[GAP]GH':\", hf_tokenizer(\"ACD[GAP]GH\", add_special_tokens=False))\n",
    "# print(\"Decoding:\", hf_tokenizer.decode([6, 7, 8, 5, 11, 12]))\n",
    "print(\"Vocab size:\", len(hf_tokenizer))\n",
    "\n",
    "hf_tokenizer.save_pretrained(\"phylo_char_tokenizer_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ca0c1-fd0e-4769-9676-5f70f64689d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import parasail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a2992-b160-4b69-ba90-0f3ea138c2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/as12267/.conda/envs/huggingface_bert/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196e974-04e2-4699-bf2d-24311f9cf39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python310.zip',\n",
       " '/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python3.10',\n",
       " '/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python3.10/lib-dynload',\n",
       " '/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "[p for p in sys.path if \"conda\" in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64581b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_as_parameter_', '_cigar', '_traceback', '_traceback_args', 'cigar', 'end_query', 'end_ref', 'get_cigar', 'get_traceback', 'len_query', 'len_ref', 'length', 'length_col', 'length_row', 'length_table', 'matches', 'matches_col', 'matches_row', 'matches_table', 'matrix', 'pointer', 'query', 'ref', 'saturated', 'score', 'score_col', 'score_row', 'score_table', 'similar', 'similar_col', 'similar_row', 'similar_table', 'traceback']\n"
     ]
    }
   ],
   "source": [
    "import parasail\n",
    "result = parasail.nw_trace_scan_16(\"AAAA\", \"AAAA\", 5, 1, parasail.blosum62)\n",
    "\n",
    "print(dir(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3ba79-3bce-41a4-b363-bc8d4c636e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq1 Aligned: MKTAYIAKQRQISFVKSHFSRQLEERLGLIE\n",
      "Seq2 Aligned: MKTVYIAKRRQISFLKSHFSRQLDDRLGLIE\n"
     ]
    }
   ],
   "source": [
    "import parasail\n",
    "\n",
    "matrix = parasail.blosum62\n",
    "\n",
    "def align_pair(seq1, seq2):\n",
    "    # global Needlemanâ€“Wunsch alignment with traceback\n",
    "    result = parasail.nw_trace_scan_16(seq1, seq2, 10, 1, matrix)\n",
    "\n",
    "    # Extract the traceback object\n",
    "    tb = result.traceback\n",
    "\n",
    "    # Get aligned sequences\n",
    "    # a1 = tb.query.replace(\"-\", \"<GAP>\")\n",
    "    # a2 = tb.ref.replace(\"-\", \"<GAP>\")\n",
    "    a1 = tb.query\n",
    "    a2 = tb.ref\n",
    "\n",
    "    return a1, a2\n",
    "\n",
    "# Test sequences\n",
    "seq1 = \"MKTAYIAKQRQISFVKSHFSRQLEERLGLIE\"\n",
    "seq2 = \"MKTVYIAKRRQISFLKSHFSRQLDDRLGLIE\"\n",
    "\n",
    "a1, a2 = align_pair(seq1, seq2)\n",
    "\n",
    "print(\"Seq1 Aligned:\", a1)\n",
    "print(\"Seq2 Aligned:\", a2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq1 Aligned: MKTAYIAKQRQISFVKSHF--S\n",
      "Seq2 Aligned: MKTA----QRQISFVKSHFSSS\n",
      "Seq1 Aligned with [GAP]: MKTAYIAKQRQISFVKSHF[GAP][GAP]S\n",
      "Seq2 Aligned with [GAP]: MKTA[GAP][GAP][GAP][GAP]QRQISFVKSHFSSS\n"
     ]
    }
   ],
   "source": [
    "from gLM.sequences import align_pair\n",
    "seq1 = \"MKTAYIAKQRQISFVKSHFS\"\n",
    "seq2 = \"MKTA---QRQISFVKSHFSSS\"\n",
    "\n",
    "a1, a2 = align_pair(seq1, seq2)\n",
    "\n",
    "print(\"Seq1 Aligned:\", a1)\n",
    "print(\"Seq2 Aligned:\", a2)\n",
    "\n",
    "print(f\"Seq1 Aligned with [GAP]:\", a1.replace(\"-\", \"[GAP]\"))\n",
    "print(f\"Seq2 Aligned with [GAP]:\", a2.replace(\"-\", \"[GAP]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    ")\n",
    "\n",
    "class PhyloTokenizerLoader:\n",
    "    def __init__(self, tokenizer_path):\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    def encode(self, aligned_seq1, aligned_seq2):\n",
    "        inputs = self.tokenizer(\n",
    "            aligned_seq2,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            aligned_seq1,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        return{\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"labels\": labels[\"input_ids\"], \n",
    "            \"attention_mask\": inputs[\"attention_mask\"]\n",
    "        }\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.tokenizer(*args, **kwargs)\n",
    "\n",
    "# Set breakpoint inside encode() method\n",
    "# Then run this:\n",
    "phylo_tokenizer = PhyloTokenizerLoader(\"phylo_char_tokenizer\")\n",
    "\n",
    "# When you call encode, debugger will pause at your breakpoint\n",
    "result = phylo_tokenizer.encode(\"ATCG\", \"ATCG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc63dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6, 22, 7, 11],\n",
       " 'labels': [6, 22, 7, 11],\n",
       " 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gLM.dataset import UniRefClusterIterableDataset\n",
    "from gLM.tokenizers import PhyloTokenizerLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PhyloTokenizerLoader(\"./phylo_char_tokenizer\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9e975",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "UniRefClusterIterableDataset.__init__() missing 1 required positional argument: 'index_db_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mUniRefClusterIterableDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfasta_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gpfs/data/brandeslab/Data/uniref/uniref100.fasta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: UniRefClusterIterableDataset.__init__() missing 1 required positional argument: 'index_db_path'"
     ]
    }
   ],
   "source": [
    "ds = UniRefClusterIterableDataset(\n",
    "    parquet_path=\"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\",\n",
    "    fasta_path=\"/gpfs/data/brandeslab/Data/uniref/uniref100.fasta\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=8192,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(ds):\n",
    "    print(\"Got item\", i)\n",
    "    if i == 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Instantiating dataset...\n",
      "Dataset constructed in 0.04993486404418945 sec\n",
      "Fetching first item...\n",
      "In the Uniref Iterable - MLM branch\n",
      "Got first item!\n",
      "First item fetched in 0.4544064998626709 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from gLM.dataset import UniRefClusterIterableDataset\n",
    "from gLM.tokenizers import PhyloTokenizerLoader\n",
    "\n",
    "parquet = \"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\"\n",
    "fasta   = \"/gpfs/data/brandeslab/Data/uniref/uniref100.fasta\"\n",
    "index_db_path = \"/gpfs/data/brandeslab/User/as12267/uniref100.idx\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tok = PhyloTokenizerLoader(\"./phylo_char_tokenizer\").load()\n",
    "\n",
    "print(\"Instantiating dataset...\")\n",
    "start = time.time()\n",
    "ds = UniRefClusterIterableDataset(\n",
    "    parquet_path=parquet,\n",
    "    index_db_path=index_db_path,\n",
    "    fasta_path=fasta,\n",
    "    tokenizer=tok,\n",
    "    max_seq_len=8192,\n",
    "    training_type=\"MLM\"\n",
    ")\n",
    "print(\"Dataset constructed in\", time.time() - start, \"sec\")\n",
    "\n",
    "print(\"Fetching first item...\")\n",
    "start = time.time()\n",
    "for i, item in enumerate(ds):\n",
    "    print(\"Got first item!\")\n",
    "    break\n",
    "print(\"First item fetched in\", time.time() - start, \"sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e777ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Fetching first item...\n",
      "REP: A0ABS9P3Y9_9GAMM MEMBERS: [] <class 'list'>\n",
      "REP: A0A835XQ69_9CHLO MEMBERS: ['A0A835XMW2_9CHLO'] <class 'list'>\n",
      "PAIR: A0A835XQ69_9CHLO A0A835XMW2_9CHLO\n",
      "REP: UPI000C86E4D3 MEMBERS: [] <class 'list'>\n",
      "REP: UPI001260CD7F MEMBERS: [] <class 'list'>\n",
      "REP: A0A2I1HB31_9GLOM MEMBERS: [] <class 'list'>\n",
      "REP: A0AAF0EAU2_9BASI MEMBERS: [] <class 'list'>\n",
      "REP: UPI002ECFBE8A MEMBERS: [] <class 'list'>\n",
      "REP: UPI0002491D06 MEMBERS: [] <class 'list'>\n",
      "REP: UPI002625D505 MEMBERS: [] <class 'list'>\n",
      "REP: A0A917ZIC9_9GAMM MEMBERS: [] <class 'list'>\n",
      "REP: UPI00188B8639 MEMBERS: [] <class 'list'>\n",
      "REP: UPI003216E2CD MEMBERS: [] <class 'list'>\n",
      "REP: A0A1R1K1Z6_ALCXX MEMBERS: ['UPI0029A3963B', 'UPI0005F94960'] <class 'list'>\n",
      "PAIR: UPI0005F94960 A0A1R1K1Z6_ALCXX\n",
      "REP: A0ABU7LKA1_9NOCA MEMBERS: ['UPI000B257CD9', 'UPI00364A542B'] <class 'list'>\n",
      "PAIR: UPI000B257CD9 A0ABU7LKA1_9NOCA\n",
      "REP: UPI00112C5A48 MEMBERS: [] <class 'list'>\n",
      "REP: A0A1I2CAG8_9RHOB MEMBERS: [] <class 'list'>\n",
      "REP: A0A386KA30_9CAUD MEMBERS: [] <class 'list'>\n",
      "REP: A0AA38XHM7_9EURO MEMBERS: [] <class 'list'>\n",
      "REP: UPI003D9F7772 MEMBERS: ['UPI003D9DFE4C', 'UPI003D9F29E3', 'UPI003D9DF066', 'UPI003D9DECA5', 'UPI003D9E7CE5', 'UPI003D9DCD8F', 'UPI003D9F0598', 'UPI003D9EA93F', 'UPI003D9EF4F1', 'UPI003D9F45C3', 'UPI003D9E7C5D', 'UPI003D9F602A', 'UPI003D9DF05F', 'UPI003D9E0940', 'UPI003D9E8C64', 'UPI003D9F0081', 'UPI003D9EBEA1', 'UPI003D9F5A30', 'UPI003D9F333C'] <class 'list'>\n",
      "PAIR: UPI003D9F5A30 UPI003D9DECA5\n",
      "REP: A0ABP9UC87_9DEIO MEMBERS: ['UPI0031E9DB90'] <class 'list'>\n",
      "PAIR: A0ABP9UC87_9DEIO UPI0031E9DB90\n",
      "REP: UPI0036289435 MEMBERS: [] <class 'list'>\n",
      "REP: A0A2P5Y146_GOSBA MEMBERS: [] <class 'list'>\n",
      "REP: UPI00187BD009 MEMBERS: ['UPI00187CD83C', 'UPI00187C5C20', 'UPI00187C9A69', 'UPI00187C4E25'] <class 'list'>\n",
      "PAIR: UPI00187C9A69 UPI00187C5C20\n",
      "SEQ1: MSAWQVWEKGQAPGSWRTRPQQQHPLDTPRPRFDIAQMHTARADRLEKCMSSMQMGTQMVKLRGGSKGLVRFFYLDEHKSCIRWRPSRKNEKAKISIDSIREVCEGKQSEIFQRYSEGSFNPNCCFSLYYGEHMESLDLVSSTGEEARTWITGLKYLMAGISDEDSLAKRQRTRDQWLKQTFTEADKNGDGSLSISEVLQLLHKLNVNLPRQKVKQMFKEADTDDNQGTLGFEEFCSFYKMMSTRRDLYLLMLTYSNLKDHLDTDNLARFLETEQKMTKVTKDHCLEIINKFEPCSENQKQGVLGIDGFTNYMRSPAGDIFNPQHYNVNQDMTQPLCNYFIASSHNTYLMGDQLMSQSRVDMYAWVLQAGCRCVEVDCWDGQDGEPIVHHGYTLTSKILFKDVIETINKYAFVKNDYPVILSIENHCSVPQQKKMAQYLIEILGDKLDLSNIKADESGWLPSPETLRGKILVKGKKLPPNIDENAEEGDVSDEDSADEMEDDCKLMNGDTSANRKQVENMAKKKLDNLMKESKIRDREDPDSFTIAALPPAGKPTDKTGSKGKSEDGTDTADEANPSSNKRTGRSFIGSFSKRKQKKTSKLKKTSSFEDTDTDQESTSSTSRAPLHHSKKKKTMKLSRALSDLVKYTRSVGLYDIEAQANCSWQVSSLSETKAHQVMQQKASSFIHFNQQQLSRIYPSSYRVDSSNFNPQPFWNAGCQLVALNYQSEGRVLQLNRAKFYSNGNCGYILKPTCMCEGDFNPNLEDPLPGQMKKQLVLKVISGQQLPKPKDSMLGDRGEIIDPFVEVEIIGLPVDCCKEQTRVVDDNGFNPMWEETLVFTVHMPELALVRFLVWDHDPIGQDFIGQRTIAFNSMMPGYRHVYLEGMEEASVFVHVAVNDITGKARTASGIKGLFHRNPKQASLDSHAAVQHSRKHPFGAHLLRRTASAPTKGQPKIKKGFPEIAIDTKDYSSEGASEEPDSEDKASTATSQQPTSPLHHNGDTLASHQTKGPWDHPDTNGAFHPEKGKDCSTVQRPAPPFLALNSEDPRNTHVICSVSTPGKSLSLHQSSSAPLHSASTAINTDPPLTVPSNGVVDSPIKVSRDPKKPPTLSTTTSSTLMNPVEKTGPSQSAQNIETYRSKHSFMEHMQNQKQNLETLTHKKNDPADLKFDRRTEPCAAACVSTGTPQVPAADIAQTRRALFSSTPVRRTKSEGHVQVAQSAVPEVCTDATMNDRLWSKLEPGSHRDSMSSSSSISSSDTVIDLSLPNLARKSLTSLHTAGSTCDPPWVNCRRSALVSYDALRVSKSKSNPNLQQHECPKDELKPRPLQLPKEDTMDSPSRLTQRRHTWSRLYMEGLKQSSASRPSSAATTPTTTASLSKSLGDLTSDDISCNFDSKYRSISRSFIVRPTREQIHKGSSLKSRPSSNLTEQLRKLTNVEPLTASDFAHENRPRESQEETVDETLVRRTSSRSQSRVRYIANRAKKAQERQRLQGLIQGRSASFSLTGSIGDGSSASPIEERGNPEGACCVAQSPCTSLDLLSQLTPLGTPSPRQSQSSPDPENSEVFFMLKL\n",
      "Len SEQ1: 1570\n",
      "SEQ2: MDSVAARMAMVTDSPNLSLRSRTTMTTDGDSFVSSSSLPCSHFLSANVINVVEKCMSSMQMGTQMVKLRGGSKGLVRFFYLDEHKSCIRWRPSRKNEKAKISIDSIREVCEGKQSEIFQRYSEGSFNPNCCFSLYYGEHMESLDLVSSTGEEARTWITGLKYLMAGISDEDSLAKRQRTRDQWLKQTFTEADKNGDGSLSISEVLQLLHKLNVNLPRQKVKQMFKEADTDDNQGTLGFEEFCSFYKMMSTRRDLYLLMLTYSNLKDHLDTDNLARFLETEQKMTKVTKDHCLEIINKFEPCSENQKQGVLGIDGFTNYMRSPAGDIFNPQHYNVNQDMTQPLCNYFIASSHNTYLMGDQLMSQSRVDMYAWVLQAGCRCVEVDCWDGQDGEPIVHHGYTLTSKILFKDVIETINKYAFVKNDYPVILSIENHCSVPQQKKMAQYLIEILGDKLDLSNIKADESGWLPSPETLRGKILVKGKKLPPNIDENAEEGDVSDEDSADEMEDDCKLMNGDTSANRKQVENMAKKKLDNLMKESKIRDREDPDSFTIAALPPAGKPTDKTGSKGKSEDGTDTADEANPSSNKRTGRSFIGSFSKRKKKTSKLKKTSSFEDTDTDQESTSSTSRAPLHHSKKKKTMKLSRALSDLVKYTRSVGLYDIEAQANCSWQVSSLSETKAHQVMQQKASSFIHFNQQQLSRIYPSSYRVDSSNFNPQPFWNAGCQLVALNYQSEGRVLQLNRAKFYSNGNCGYILKPTCMCEGDFNPNLEDPLPGQMKKQLVLKVISGQQLPKPKDSMLGDRGEIIDPFVEVEIIGLPVDCCKEQTRVVDDNGFNPMWEETLVFTVHMPELALVRFLVWDHDPIGQDFIGQRTIAFNSMMPGYRHVYLEGMEEASVFVHVAVNDITGKARTASGIKGLFHRNPKQASLDSHAAVQHSRKHPFGAHLLRRTASAPTKGQPKIKKGFPEIAIDTKDYSSEGASEEPDSEDKASTATSQQPTSPLHHNGDTLASHQTKGPWDHPDTNGAFHPEKGKDCSTVQRPAPPFLALNSEDPRNTHVICSVSTPGKSLSLHQSSSAPLHSASTAINTDPPLTVPSNGVVDSPIKVSRDPKKPPTLSTTTSSTLMNPVEKTGPSQSAQNIETYRSKHSFMEHMQNQKQNLETLTHKKNDPADLKFDRRTEPCAAACVSTGTPQVPAADIAQTRRALFSSTPVRRTKSEGHVQVAQSAVPEVCTDATMNDRLWSKLEPGSHRDSMSSSSSISSSDTVIDLSLPNLARKSLTSLHTAGSTCDPPWVNCRRSALVSYDALRVSKSKSNPNLQQHECPKDELKPRPLQLPKEDTMDSPSRLTQRRHTWSRLYMEGLKQSSASRPSSAATTPTTTASLSKSLGDLTSDDISCNFDSKYRSISRSFIVRPTREQIHKGSSLKSRPSSNLTEQLRKLTNVEPLTASDFAHENRPRESQEETVDETLVRRTSSRSQSRVRYIANRAKKAQERQRLQGLIQGRSASFSLTGSIGDGSSASPIEERGNPEGACCVAQSPCTSLDLLSQLTPLGTPSPRQSQSSPDPENSEVFFMLKL\n",
      "Len SEQ2: 1575\n",
      "ALN1: MS--AWQVWEKGQAPG-SWRTRPQQQHPLDTPRPRFDIAQMHTARADRL---EKCMSSMQMGTQMVKLRGGSKGLVRFFYLDEHKSCIRWRPSRKNEKAKISIDSIREVCEGKQSEIFQRYSEGSFNPNCCFSLYYGEHMESLDLVSSTGEEARTWITGLKYLMAGISDEDSLAKRQRTRDQWLKQTFTEADKNGDGSLSISEVLQLLHKLNVNLPRQKVKQMFKEADTDDNQGTLGFEEFCSFYKMMSTRRDLYLLMLTYSNLKDHLDTDNLARFLETEQKMTKVTKDHCLEIINKFEPCSENQKQGVLGIDGFTNYMRSPAGDIFNPQHYNVNQDMTQPLCNYFIASSHNTYLMGDQLMSQSRVDMYAWVLQAGCRCVEVDCWDGQDGEPIVHHGYTLTSKILFKDVIETINKYAFVKNDYPVILSIENHCSVPQQKKMAQYLIEILGDKLDLSNIKADESGWLPSPETLRGKILVKGKKLPPNIDENAEEGDVSDEDSADEMEDDCKLMNGDTSANRKQVENMAKKKLDNLMKESKIRDREDPDSFTIAALPPAGKPTDKTGSKGKSEDGTDTADEANPSSNKRTGRSFIGSFSKRKQKKTSKLKKTSSFEDTDTDQESTSSTSRAPLHHSKKKKTMKLSRALSDLVKYTRSVGLYDIEAQANCSWQVSSLSETKAHQVMQQKASSFIHFNQQQLSRIYPSSYRVDSSNFNPQPFWNAGCQLVALNYQSEGRVLQLNRAKFYSNGNCGYILKPTCMCEGDFNPNLEDPLPGQMKKQLVLKVISGQQLPKPKDSMLGDRGEIIDPFVEVEIIGLPVDCCKEQTRVVDDNGFNPMWEETLVFTVHMPELALVRFLVWDHDPIGQDFIGQRTIAFNSMMPGYRHVYLEGMEEASVFVHVAVNDITGKARTASGIKGLFHRNPKQASLDSHAAVQHSRKHPFGAHLLRRTASAPTKGQPKIKKGFPEIAIDTKDYSSEGASEEPDSEDKASTATSQQPTSPLHHNGDTLASHQTKGPWDHPDTNGAFHPEKGKDCSTVQRPAPPFLALNSEDPRNTHVICSVSTPGKSLSLHQSSSAPLHSASTAINTDPPLTVPSNGVVDSPIKVSRDPKKPPTLSTTTSSTLMNPVEKTGPSQSAQNIETYRSKHSFMEHMQNQKQNLETLTHKKNDPADLKFDRRTEPCAAACVSTGTPQVPAADIAQTRRALFSSTPVRRTKSEGHVQVAQSAVPEVCTDATMNDRLWSKLEPGSHRDSMSSSSSISSSDTVIDLSLPNLARKSLTSLHTAGSTCDPPWVNCRRSALVSYDALRVSKSKSNPNLQQHECPKDELKPRPLQLPKEDTMDSPSRLTQRRHTWSRLYMEGLKQSSASRPSSAATTPTTTASLSKSLGDLTSDDISCNFDSKYRSISRSFIVRPTREQIHKGSSLKSRPSSNLTEQLRKLTNVEPLTASDFAHENRPRESQEETVDETLVRRTSSRSQSRVRYIANRAKKAQERQRLQGLIQGRSASFSLTGSIGDGSSASPIEERGNPEGACCVAQSPCTSLDLLSQLTPLGTPSPRQSQSSPDPENSEVFFMLKL\n",
      "Len ALN1: 1576\n",
      "ALN2: MDSVAARMAMVTDSPNLSLRSRTTMTTDGDSFVSSSSLPCSHFLSANVINVVEKCMSSMQMGTQMVKLRGGSKGLVRFFYLDEHKSCIRWRPSRKNEKAKISIDSIREVCEGKQSEIFQRYSEGSFNPNCCFSLYYGEHMESLDLVSSTGEEARTWITGLKYLMAGISDEDSLAKRQRTRDQWLKQTFTEADKNGDGSLSISEVLQLLHKLNVNLPRQKVKQMFKEADTDDNQGTLGFEEFCSFYKMMSTRRDLYLLMLTYSNLKDHLDTDNLARFLETEQKMTKVTKDHCLEIINKFEPCSENQKQGVLGIDGFTNYMRSPAGDIFNPQHYNVNQDMTQPLCNYFIASSHNTYLMGDQLMSQSRVDMYAWVLQAGCRCVEVDCWDGQDGEPIVHHGYTLTSKILFKDVIETINKYAFVKNDYPVILSIENHCSVPQQKKMAQYLIEILGDKLDLSNIKADESGWLPSPETLRGKILVKGKKLPPNIDENAEEGDVSDEDSADEMEDDCKLMNGDTSANRKQVENMAKKKLDNLMKESKIRDREDPDSFTIAALPPAGKPTDKTGSKGKSEDGTDTADEANPSSNKRTGRSFIGSFSKRK-KKTSKLKKTSSFEDTDTDQESTSSTSRAPLHHSKKKKTMKLSRALSDLVKYTRSVGLYDIEAQANCSWQVSSLSETKAHQVMQQKASSFIHFNQQQLSRIYPSSYRVDSSNFNPQPFWNAGCQLVALNYQSEGRVLQLNRAKFYSNGNCGYILKPTCMCEGDFNPNLEDPLPGQMKKQLVLKVISGQQLPKPKDSMLGDRGEIIDPFVEVEIIGLPVDCCKEQTRVVDDNGFNPMWEETLVFTVHMPELALVRFLVWDHDPIGQDFIGQRTIAFNSMMPGYRHVYLEGMEEASVFVHVAVNDITGKARTASGIKGLFHRNPKQASLDSHAAVQHSRKHPFGAHLLRRTASAPTKGQPKIKKGFPEIAIDTKDYSSEGASEEPDSEDKASTATSQQPTSPLHHNGDTLASHQTKGPWDHPDTNGAFHPEKGKDCSTVQRPAPPFLALNSEDPRNTHVICSVSTPGKSLSLHQSSSAPLHSASTAINTDPPLTVPSNGVVDSPIKVSRDPKKPPTLSTTTSSTLMNPVEKTGPSQSAQNIETYRSKHSFMEHMQNQKQNLETLTHKKNDPADLKFDRRTEPCAAACVSTGTPQVPAADIAQTRRALFSSTPVRRTKSEGHVQVAQSAVPEVCTDATMNDRLWSKLEPGSHRDSMSSSSSISSSDTVIDLSLPNLARKSLTSLHTAGSTCDPPWVNCRRSALVSYDALRVSKSKSNPNLQQHECPKDELKPRPLQLPKEDTMDSPSRLTQRRHTWSRLYMEGLKQSSASRPSSAATTPTTTASLSKSLGDLTSDDISCNFDSKYRSISRSFIVRPTREQIHKGSSLKSRPSSNLTEQLRKLTNVEPLTASDFAHENRPRESQEETVDETLVRRTSSRSQSRVRYIANRAKKAQERQRLQGLIQGRSASFSLTGSIGDGSSASPIEERGNPEGACCVAQSPCTSLDLLSQLTPLGTPSPRQSQSSPDPENSEVFFMLKL\n",
      "Len ALN2: 1576\n",
      "TRUNC1: MS--AWQVWEKGQAPG-SWRTRPQQQHPLDTPRPRFDIAQMHTARADRL---EKCMSSMQMGTQMVKLRGGSKGLVRFFYLDEHKSCIRWRPSRKNEKAKISIDSIREVCEGKQSEIFQRYSEGSFNPNCCFSLYYGEHMESLDLVSSTGEEARTWITGLKYLMAGISDEDSLAKRQRTRDQWLKQTFTEADKNGDGSLSISEVLQLLHKLNVNLPRQKVKQMFKEADTDDNQGTLGFEEFCSFYKMMSTRRDLYLLMLTYSNLKDHLDTDNLARFLETEQKMTKVTKDHCLEIINKFEPCSENQKQGVLGIDGFTNYMRSPAGDIFNPQHYNVNQDMTQPLCNYFIASSHNTYLMGDQLMSQSRVDMYAWVLQAGCRCVEVDCWDGQDGEPIVHHGYTLTSKILFKDVIETINKYAFVKNDYPVILSIENHCSVPQQKKMAQYLIEILGDKLDLSNIKADESGWLPSPETLRGKILVKGKKLPPNIDENAEEGDVSDEDSADEMEDDCKLMNGDTSANRKQVENMAKKKLDNLMKESKIRDREDPDSFTIAALPPAGKPTDKTGSKGKSEDGTDTADEANPSSNKRTGRSFIGSFSKRKQKKTSKLKKTSSFEDTDTDQESTSSTSRAPLHHSKKKKTMKLSRALSDLVKYTRSVGLYDIEAQANCSWQVSSLSETKAHQVMQQKASSFIHFNQQQLSRIYPSSYRVDSSNFNPQPFWNAGCQLVALNYQSEGRVLQLNRAKFYSNGNCGYILKPTCMCEGDFNPNLEDPLPGQMKKQLVLKVISGQQLPKPKDSMLGDRGEIIDPFVEVEIIGLPVDCCKEQTRVVDDNGFNPMWEETLVFTVHMPELALVRFLVWDHDPIGQDFIGQRTIAFNSMMPGYRHVYLEGMEEASVFVHVAVNDITGKARTASGIKGLFHRNPKQASLDSHAAVQHSRKHPFGAHLLRRTASAPTKGQPKIKKGFPEIAIDTKDYSSEGASEEPDSEDKASTATSQQPTSPLHHNGDTLASHQTKGPWDHPDTNGAFHPEKGKDCSTVQRPAPPFLALNSEDPRNTHVICSVSTPGKSLSLHQSSSAPLHSASTAINTDPPLTVPSNGVVDSPIKVSRDPKKPPTLSTTTSSTLMNPVEKTGPSQSAQNIETYRSKHSFMEHMQNQKQNLETLTHKKNDPADLKFDRRTEPCAAACVSTGTPQVPAADIAQTRRALFSSTPVRRTKSEGHVQVAQSAVPEVCTDATMNDRLWSKLEPGSHRDSMSSSSSISSSDTVIDLSLPNLARKSLTSLHTAGSTCDPPWVNCRRSALVSYDALRVSKSKSNPNLQQHECPKDELKPRPLQLPKEDTMDSPSRLTQRRHTWSRLYMEGLKQSSASRPSSAATTPTTTASLSKSLGDLTSDDISCNFDSKYRSISRSFIVRPTREQIHKGSSLKSRPSSNLTEQLRKLTNVEPLTASDFAHENRPRESQEETVDETLVRRTSSRSQSRVRYIANRAKKAQERQRLQGLIQGRSASFSLTGSIGDGSSASPIEERGNPEGACCVAQSPCTSLDLLSQLTPLGTPSPRQSQSSPDPENSEVFFMLKL\n",
      "Len TRUNC1: 1576\n",
      "TRUNC2: MDSVAARMAMVTDSPNLSLRSRTTMTTDGDSFVSSSSLPCSHFLSANVINVVEKCMSSMQMGTQMVKLRGGSKGLVRFFYLDEHKSCIRWRPSRKNEKAKISIDSIREVCEGKQSEIFQRYSEGSFNPNCCFSLYYGEHMESLDLVSSTGEEARTWITGLKYLMAGISDEDSLAKRQRTRDQWLKQTFTEADKNGDGSLSISEVLQLLHKLNVNLPRQKVKQMFKEADTDDNQGTLGFEEFCSFYKMMSTRRDLYLLMLTYSNLKDHLDTDNLARFLETEQKMTKVTKDHCLEIINKFEPCSENQKQGVLGIDGFTNYMRSPAGDIFNPQHYNVNQDMTQPLCNYFIASSHNTYLMGDQLMSQSRVDMYAWVLQAGCRCVEVDCWDGQDGEPIVHHGYTLTSKILFKDVIETINKYAFVKNDYPVILSIENHCSVPQQKKMAQYLIEILGDKLDLSNIKADESGWLPSPETLRGKILVKGKKLPPNIDENAEEGDVSDEDSADEMEDDCKLMNGDTSANRKQVENMAKKKLDNLMKESKIRDREDPDSFTIAALPPAGKPTDKTGSKGKSEDGTDTADEANPSSNKRTGRSFIGSFSKRK-KKTSKLKKTSSFEDTDTDQESTSSTSRAPLHHSKKKKTMKLSRALSDLVKYTRSVGLYDIEAQANCSWQVSSLSETKAHQVMQQKASSFIHFNQQQLSRIYPSSYRVDSSNFNPQPFWNAGCQLVALNYQSEGRVLQLNRAKFYSNGNCGYILKPTCMCEGDFNPNLEDPLPGQMKKQLVLKVISGQQLPKPKDSMLGDRGEIIDPFVEVEIIGLPVDCCKEQTRVVDDNGFNPMWEETLVFTVHMPELALVRFLVWDHDPIGQDFIGQRTIAFNSMMPGYRHVYLEGMEEASVFVHVAVNDITGKARTASGIKGLFHRNPKQASLDSHAAVQHSRKHPFGAHLLRRTASAPTKGQPKIKKGFPEIAIDTKDYSSEGASEEPDSEDKASTATSQQPTSPLHHNGDTLASHQTKGPWDHPDTNGAFHPEKGKDCSTVQRPAPPFLALNSEDPRNTHVICSVSTPGKSLSLHQSSSAPLHSASTAINTDPPLTVPSNGVVDSPIKVSRDPKKPPTLSTTTSSTLMNPVEKTGPSQSAQNIETYRSKHSFMEHMQNQKQNLETLTHKKNDPADLKFDRRTEPCAAACVSTGTPQVPAADIAQTRRALFSSTPVRRTKSEGHVQVAQSAVPEVCTDATMNDRLWSKLEPGSHRDSMSSSSSISSSDTVIDLSLPNLARKSLTSLHTAGSTCDPPWVNCRRSALVSYDALRVSKSKSNPNLQQHECPKDELKPRPLQLPKEDTMDSPSRLTQRRHTWSRLYMEGLKQSSASRPSSAATTPTTTASLSKSLGDLTSDDISCNFDSKYRSISRSFIVRPTREQIHKGSSLKSRPSSNLTEQLRKLTNVEPLTASDFAHENRPRESQEETVDETLVRRTSSRSQSRVRYIANRAKKAQERQRLQGLIQGRSASFSLTGSIGDGSSASPIEERGNPEGACCVAQSPCTSLDLLSQLTPLGTPSPRQSQSSPDPENSEVFFMLKL\n",
      "Len TRUNC2: 1576\n",
      "PID: 97.20812182741116\n",
      "Success!\n",
      "dict_keys(['input_ids', 'labels', 'attention_mask', 'percent_identity'])\n"
     ]
    }
   ],
   "source": [
    "from gLM.dataset import UniRefClusterIterableDataset\n",
    "from gLM.tokenizers import PhyloTokenizerLoader\n",
    "import time\n",
    "\n",
    "parquet = \"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\"\n",
    "fasta   = \"/gpfs/data/brandeslab/Data/uniref/uniref100.fasta\"\n",
    "index_db = \"/gpfs/data/brandeslab/User/as12267/uniref100.idx\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tok = PhyloTokenizerLoader(\"./phylo_char_tokenizer\")\n",
    "\n",
    "ds = UniRefClusterIterableDataset(\n",
    "    parquet_path=parquet,\n",
    "    index_db_path=index_db,\n",
    "    fasta_path=fasta,\n",
    "    tokenizer=tok,\n",
    "    max_seq_len=8192,\n",
    ")\n",
    "\n",
    "print(\"Fetching first item...\")\n",
    "for i, x in enumerate(ds):\n",
    "    print(\"Success!\")\n",
    "    print(x.keys())\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd7ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gLM.tokenizers.phylo_tokenizer.PhyloTokenizerLoader object at 0x15543fe5d510>\n",
      "[MASK]\n",
      "0\n",
      "5\n",
      "{'[UNK]': 1, 'C': 7, '[MASK]': 4, 'T': 22, '[SEP]': 3, '[CLS]': 2, 'H': 12, 'E': 9, 'L': 15, 'F': 10, 'G': 11, 'Y': 25, '[PAD]': 0, 'Q': 19, 'K': 14, 'A': 6, '-': 5, 'P': 18, 'I': 13, 'W': 24, 'R': 20, 'D': 8, 'S': 21, 'M': 16, 'N': 17, 'V': 23}\n",
      "{'input_ids': [6, 7, 8, 5, 9, 10, 11], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from gLM.tokenizers import PhyloTokenizerLoader\n",
    "\n",
    "tok = PhyloTokenizerLoader(\"./phylo_char_tokenizer\")\n",
    "\n",
    "print(tok)                    # should show PreTrainedTokenizerFast\n",
    "print(tok.mask_token)         # should NOT raise error\n",
    "print(tok.pad_token_id)\n",
    "print(tok.convert_tokens_to_ids(\"-\"))\n",
    "print(tok.get_vocab())\n",
    "print(tok(\"ACD-EFG\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb149197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phylo Tokenizer loaded\n",
      "Mask ID: 4\n",
      "Non-GAP ID: 6\n",
      "GAP ID: 5\n",
      "Tokenizer vocab size: 27\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PhyloTokenizerLoader(\"./phylo_char_tokenizer_updated\")\n",
    "pad_id = tokenizer.pad_token_id\n",
    "mask_id = tokenizer.mask_token_id\n",
    "non_gap_id = tokenizer.convert_tokens_to_ids(\"-\") \n",
    "gap_id = tokenizer.convert_tokens_to_ids(\"[GAP]\")\n",
    "print(\"Phylo Tokenizer loaded\")\n",
    "print(\"Mask ID:\", mask_id)\n",
    "print(\"Non-GAP ID:\", non_gap_id)\n",
    "print(\"GAP ID:\", gap_id)\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ade34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "In the Uniref Iterable - Phylo branch\n",
      "In the Uniref Iterable - Phylo branch\n",
      "In the Uniref Iterable - Phylo branch\n",
      "Untokenize seq A1 MDTEGFPRQFARTRRFSLGVPREFTVSPDGDRVLFLRSESGVVPRVHLWMYESGGERILTDPA[GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP]AGVGTYATDRHVRVVAYTVDGSLWTVRTDGGLPRRIQTVGPVRDPRPSPDGTLIAYVTGGALRVVGTDGAGDRPLAEPETAETTYGLADYSAVASIGRSRGYWWSPDSGALLVARVDTSVVERRYLSDPSDPGQSPRSVRYPAAGTANAITSLHLVTVAGGHTPVRLPRQAPAKDAPADSWGLAFEYVVGADWQSGGPVISLQTRDQRTMWVLRVDPVHGSVEQLSRQTDQGWVEFPPGAPLHTASGVLVLPQVRGDERTIRIGGVFAPAGLQVRAVLGSVGEKVWFAASEEPTEVHVWSYEAGHGFERLTQTPGVHTATAGGDTLVLDSRTLGGHAVTVLRDGKQVGCIAVLAEQPLVTPRPVHLTLGKRQLRSRLHLPSWYEPGMAKLPVLLSPYAGPGMQVVTKAHGWYTAVCQWYAEQGFAVLATDGRGTPGRGVRWQRAILGDRLTPVLDDQIDGLHAAARRCDALDLERVGIRGWSFSGYLAAGAVLHRPDVFHAAVAGATPTDRRLYDTYWEERFLGHPDLQPHNYERSSLLPLAEKLTRPLMLVHGLADDNVAPAHTLRLSAELLAAGRPHRVLLLPGVGHLVTGEGVADTLLQLELDFLKSSLGA, length A1: 792\n",
      "Untokenize length A1: 792\n",
      "Untokenize seq A2 MDTEGFPRQFARTRRFSLGVPREFTVSPDGDRVLFLRSESGVVPRVQLWMYKSGEERILADPAVAGGEEEAMEAERIRPESNRANSAGVQSYATDRDVRVVAYTVDGSLWAVRTDGSLPRRIQTVGPVRDPRPSPDGTLIAYVTGGALRVVGTDGAGDRLLAEPETAETTYGLADYSAVASIGRSRGYWWSPDSDALLVARVDTSVVERRYLSDPSNLSQLPRSIRYPAAGTANAITSLHLVTVAGEHTPVRLPRQAPAKEAPANSWGLAFEYVVSADWQSSGPVISLQTRDQRTMWVLRVNPVDGSVEQLSQQADEDWVEFPPGAPLYTASGVLVLPRVRGDERTIRIGGVFAPAGLHVRAVLGSVGEKVLFAASEEPTEVHVWSYEAGHGFERLTQTPGVHTATAGGDTLVLDSRTLDGHTVTVLRGGKQVGCIAVLAEHPLVTPRPVHLTVGKRQLRSRLHLPSWYEPGMAKLPVLLSPYAGPGMQVVTKAHGWYTAVCQWYAEQGFAVLATDGRGTPGRGVQWQRAILGDRLTPVLDDQIDGLHAAAQRCDAMDLERVGIRGWSFSGYLAAGAVLHRPDVFHAAVAGAAPTDRRLYDTYWEERFLGHPDLQPHNYERSSLLSHAEKLTRPLMLVHGLADDNVAPAHTLRLSAEMLAAGRPHRVLLMPGVGHLVTGEGAADTLLQLELDFLKSSLGA\n",
      "Untokenize length A2: 700\n",
      "Token count A1: 700\n",
      "Token count A2: 700\n",
      "Max length after tokenization: 700\n",
      "In the Uniref Iterable - Phylo branch\n",
      "In the Uniref Iterable - Phylo branch\n",
      "In the Uniref Iterable - Phylo branch\n",
      "In the Uniref Iterable - Phylo branch\n",
      "Untokenize seq A1 MAGSRRTGLARSAKLASLPMGIMARRVTATGKAFMTGASRGDLDDALVEKAADEVFAVLGELKGGAMKLGQALSVAEASIPPRYADRYRAALVKLQSQAPPMSTRSVHRMLAEQLGIRWRDRFSAFDDEPVAAASIGQVHRAVWHDGRTVAVKVQYPGADTALLSDLKMLQMFSGAFARMMPGADAKGLIDEFIDRTADELDYRIEAAHQRRFALAFDGDPQFFVPKVIASAPKVLVTQWLSATPLARIITDGSKAQRDSAGLLLAEFGLSSPVRVGYLHCDPHPGNFQLLEDGRLGVIDFGAAIALPRGIPAAIGEVARHAVAGDYRTAAKTLQDNGFLRTGEQLDLAPIERLVAPIVAQIDGDNLHISRKLLQGHTARALDAKNLSMNNAMAIKAPAQWPELAMLGRTFAGVVGVCAQLDAEGPFLALVQRWLPGYAADGAAA, length A1: 445\n",
      "Untokenize length A1: 445\n",
      "Untokenize seq A2 MAGSRRTGFARSAKLASLPMGIMARRVTATGKALVTGASRGDLDDALIEKAADEVFAVLGELKGGAMKLGQALSVAEASIPPRFADRYRTALVKLQSQAPPMSTRSVHRMLTEQLGTRWRDRFSVFDDEPVAAASIGQVHRAVWHDGRTVAVKVQYPGADTALLSDLKMLQMFSGAFTKLMPGADAKGLIDEFIARTADELDYRIESAHQRRFALAFDGDPRFFVPKVIASAPKVLVTQWLEATPLAQIITGGSKAQRDSAGLLLAEFGLSSPARVGYLHCDPHPGNFQLLDDGRLGVIDFGASIALPQGIPAVIGELARHAVAEDYLAAAKTLQDNGFLRPGQQLEIEPIQRLVAPIVAQIDGDNLHISRKLLQGHTARALDVKNMSMNNAMAIKAPAQWPELAMLGRTFAGVVGVCAQLDAEGPFLGLVQRWLPGYAADGAAA\n",
      "Untokenize length A2: 445\n",
      "Token count A1: 445\n",
      "Token count A2: 445\n",
      "Max length after tokenization: 445\n",
      "In the Uniref Iterable - Phylo branch\n",
      "In the Uniref Iterable - Phylo branch\n",
      "In the Uniref Iterable - Phylo branch\n",
      "Untokenize seq A1 MSAGAAPLADNPFPVRPPDLGTHDPACPGPERQDGYIRFTGRPNPPDVAGGGVVLIGGRPHLGKSVLAHRCLHHYAAAGHVLVDLSGRAAGDIGGPVECLARMRDICALPDEVRAALREVRAAEPVEGHHEFGRALGGRRLVVRLPRPDPSLRPPTIAARVLEYARAAATANAGVYLFEFPFWLERDWGQVREGIARSADEGLVTCITLEPFSDRELMEFLWARVGGTDALDELFDLDPAVLLRSLARHRASLSPNNLTWFHVLCRQAFEAAIESDATKVAIHHYLAAAVRIGAP, length A1: 295\n",
      "Untokenize length A1: 295\n",
      "Untokenize seq A2 MNAGAAPLADNPFPVRPPDLGTHDPACPGPERQDGYIPFTGRPNPPDVAGGGVVLIGGRPHLGKSVLAHRCLHRYAAAGYVLVDLSGRAVDDIGGPVECLARMRDIRSLPDEVRAALREVRAAEPVEGHHEFGRALGGRRLVVRLPRPDPSLRPPTIAARVLEYARAAATANAGVYLFEFPFWLERDWGQVREGIARSADEGLVTCITLEPFSDRELMEFLWARVGGTDALNELFDLDPAVLLRSLARHRASLSPNNLTWFHVLCRQAFEAAIESDATKVAIHHYLAAAVRIGAP\n",
      "Untokenize length A2: 295\n",
      "Token count A1: 295\n",
      "Token count A2: 295\n",
      "Max length after tokenization: 295\n",
      "In the Uniref Iterable - Phylo branch\n",
      "Untokenize seq A1 MENFENWNKNVKKRDLNSNENANLNDFNKRDYDKEPIVIKNPYEFFLKNLDLFCFIGAFVILSTACIDYADNIHKKEIKILLYFPFVFVILHIIWAFYYYIIKNKCETKFTNKTIEFIINGEVKRVKNLTDLEPITRNFKLSYSLGVYFHLFLFFFDYFNFN, length A1: 162\n",
      "Untokenize length A1: 162\n",
      "Untokenize seq A2 MENFENWNKNVKKRDLNSNENANLSDFNQRDYDKEPIVIKNPYEFFLKNLDLFCFIGAFVILSTACIDYTDNIHTKEIKILLYFPFTFVILHIIWTFYYYIIKNKCETKFTNKTIEFVINGEVKRVKNLIDLEPITRNF[GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP][GAP]\n",
      "Untokenize length A2: 254\n",
      "Token count A1: 162\n",
      "Token count A2: 162\n",
      "Max length after tokenization: 162\n",
      "[Collator] Batch max input length: 700\n",
      "  Sample 0 input length: 700\n",
      "  Sample 1 input length: 445\n",
      "  Sample 2 input length: 295\n",
      "  Sample 3 input length: 162\n",
      "input_ids shape: torch.Size([4, 700])\n",
      "labels shape: torch.Size([4, 700])\n",
      "attention_mask shape: torch.Size([4, 700])\n",
      "percent_identity: tensor([14.1429, 91.9101, 96.9492, 81.4815])\n"
     ]
    }
   ],
   "source": [
    "from gLM.dataset import UniRefClusterIterableDataset\n",
    "from gLM.tokenizers import PhyloTokenizerLoader\n",
    "from gLM.collator import SequencePairCollator\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "parquet = \"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\"\n",
    "fasta   = \"/gpfs/data/brandeslab/Data/uniref/uniref100.fasta\"\n",
    "index_db = \"/gpfs/data/brandeslab/User/as12267/uniref100.idx\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "# tok = PhyloTokenizerLoader(\"./phylo_char_tokenizer\")\n",
    "tok = PhyloTokenizerLoader(\"./phylo_char_tokenizer_updated\")\n",
    "\n",
    "ds = UniRefClusterIterableDataset(\n",
    "    parquet_path=parquet,\n",
    "    index_db_path=index_db,\n",
    "    fasta_path=fasta,\n",
    "    tokenizer=tok,\n",
    "    max_seq_len=8192,\n",
    "    training_type=\"phylo\"\n",
    ")\n",
    "\n",
    "\n",
    "collator = SequencePairCollator(pad_id=tok.tokenizer.pad_token_id)\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds, \n",
    "    batch_size=4, \n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "batch = next(iter(loader))\n",
    "\n",
    "print(\"input_ids shape:\", batch[\"input_ids\"].shape)\n",
    "print(\"labels shape:\", batch[\"labels\"].shape)\n",
    "print(\"attention_mask shape:\", batch[\"attention_mask\"].shape)\n",
    "print(\"percent_identity:\", batch[\"percent_identity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d0d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Batch Info ---\n",
      "input_ids shape:      torch.Size([128, 2198])\n",
      "labels shape:         torch.Size([128, 2198])\n",
      "attention_mask shape: torch.Size([128, 2198])\n",
      "percent_identity:     tensor([80.3828, 98.7730, 98.1013, 98.5714, 96.6057, 95.7672, 99.6068, 92.6471,\n",
      "        99.9545, 99.0654, 96.6728, 98.1366, 99.7462, 98.2507, 92.7602, 95.2381,\n",
      "        99.0826, 99.3644, 90.8257, 92.8158, 96.7914, 94.7917, 99.2924, 93.0000,\n",
      "        98.7730, 75.1196, 93.1452, 96.5517, 90.3509, 99.9168, 99.4429, 96.6346,\n",
      "        91.4286, 99.0937, 97.2414, 91.8841, 89.7674, 92.0489, 94.0887, 98.4375,\n",
      "        91.1565, 99.3243, 88.2129, 99.0991, 92.6471, 97.5510, 97.5845, 99.5968,\n",
      "        93.1559, 98.7730, 80.6452, 98.8166, 98.1183, 91.3979, 88.0769, 91.3043,\n",
      "        97.8261, 99.4012, 74.9231, 99.4220, 93.5484, 99.6689, 99.3072, 90.2017,\n",
      "        90.1408, 94.3114, 92.0038, 99.3088, 90.7990, 89.1374, 94.8718, 95.0000,\n",
      "        99.7630, 98.9305, 97.0588, 95.5823, 99.6805, 96.9595, 91.1504, 90.5028,\n",
      "        93.4138, 92.4051, 88.7640, 97.2028, 97.6190, 97.7636, 93.8462, 97.8182,\n",
      "        92.0375, 94.8052, 99.2537, 96.8036, 99.8197, 97.6676, 96.2766, 99.8298,\n",
      "        90.1526, 99.3348, 90.7738, 93.9828, 95.9184, 92.8571, 87.0130, 86.4769,\n",
      "        99.1189, 99.2042, 97.3046, 99.8313, 98.5294, 90.4382, 97.9730, 93.2149,\n",
      "        93.9394, 90.3955, 99.8496, 89.2405, 99.8597, 90.0000, 95.8231, 96.5517,\n",
      "        95.9184, 92.1348, 91.5531, 90.4306, 98.7654, 98.1043, 99.0177, 81.5603])\n",
      "Loaded 1 batch in 31.317 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from gLM.dataset import UniRefClusterIterableDataset\n",
    "from gLM.tokenizers import PhyloTokenizerLoader\n",
    "from gLM.collator import SequencePairCollator\n",
    "\n",
    "# === File paths ===\n",
    "parquet = \"/gpfs/data/brandeslab/Data/uniref/uniref90_clusters.parquet\"\n",
    "fasta   = \"/gpfs/data/brandeslab/Data/uniref/uniref100.fasta\"\n",
    "index_db = \"/gpfs/data/brandeslab/User/as12267/uniref100.idx\"\n",
    "\n",
    "# === Load tokenizer ===\n",
    "print(\"Loading tokenizer...\")\n",
    "tok = PhyloTokenizerLoader(\"./phylo_char_tokenizer_updated\")\n",
    "\n",
    "# === Dataset ===\n",
    "ds = UniRefClusterIterableDataset(\n",
    "    parquet_path=parquet,\n",
    "    index_db_path=index_db,\n",
    "    fasta_path=fasta,\n",
    "    tokenizer=tok,\n",
    "    max_seq_len=8192,\n",
    "    training_type=\"phylo\",  # or \"MLM\"\n",
    "    batch_size=128  # this is internal batch collection, not DataLoader batch_size\n",
    ")\n",
    "\n",
    "# === Collator ===\n",
    "collator = SequencePairCollator(tok)\n",
    "\n",
    "# === Dataloader ===\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=128,  # this is the final batch size the model sees\n",
    "    collate_fn=collator,\n",
    "    num_workers=16,  # Use >0 to test multiprocessing\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# === Test: Get one batch ===\n",
    "start = time.time()\n",
    "batch = next(iter(loader))\n",
    "end = time.time()\n",
    "\n",
    "# === Inspect batch ===\n",
    "print(\"\\n--- Batch Info ---\")\n",
    "print(\"input_ids shape:     \", batch[\"input_ids\"].shape)\n",
    "print(\"labels shape:        \", batch[\"labels\"].shape)\n",
    "print(\"attention_mask shape:\", batch[\"attention_mask\"].shape)\n",
    "print(\"percent_identity:    \", batch[\"percent_identity\"])\n",
    "print(f\"Loaded 1 batch in {end - start:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617f569",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "PreTrainedTokenizerFast has no attribute load",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPhyloTokenizerLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./phylo_char_tokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m()\n\u001b[1;32m      2\u001b[0m pad_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m      3\u001b[0m gap_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/gpfs/data/brandeslab/Project/HuggingfaceTransformer/gLM/tokenizers/phylo_tokenizer.py:92\u001b[0m, in \u001b[0;36mPhyloTokenizerLoader.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/huggingface_bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1127\u001b[0m, in \u001b[0;36mSpecialTokensMixin.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(attr_as_tokens) \u001b[38;5;28;01mif\u001b[39;00m attr_as_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m-> 1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: PreTrainedTokenizerFast has no attribute load"
     ]
    }
   ],
   "source": [
    "tokenizer = PhyloTokenizerLoader(\"./phylo_char_tokenizer\").load()\n",
    "pad_id = tokenizer.pad_token_id\n",
    "gap_id = tokenizer.convert_tokens_to_ids(\"-\")\n",
    "print(\"Phylo Tokenizer loaded. GAP ID:\", gap_id)\n",
    "print(f\"vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d7770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cluster_id': 'UniRef90_A0AAD2JM23', 'representative_id': 'A0AAD2JM23_9STRA', 'members': ['A0A9K3LU73_9STRA']}\n",
      "{'cluster_id': 'UniRef90_A0A2T5GHF6', 'representative_id': 'A0A2T5GHF6_9SPHN', 'members': ['UPI0011B1CF18', 'UPI00177CA10F', 'UPI00335862B2', 'UPI001783B8B9', 'UPI00178503EB', 'UPI002A6B860E']}\n",
      "{'cluster_id': 'UniRef90_A0ABD3RNB6', 'representative_id': 'A0ABD3RNB6_9LAMI', 'members': []}\n",
      "{'cluster_id': 'UniRef90_A0ABN7WPN7', 'representative_id': 'A0ABN7WPN7_GIGMA', 'members': ['A0ABN7X348_GIGMA']}\n",
      "{'cluster_id': 'UniRef90_UPI0031F96DD0', 'representative_id': 'UPI0031F96DD0', 'members': []}\n"
     ]
    }
   ],
   "source": [
    "from gLM.data_utils.uniref_cluster_sampler import RandomClusterSampler\n",
    "\n",
    "sampler = RandomClusterSampler(parquet)\n",
    "for _ in range(5):\n",
    "    cluster = sampler.sample_clusters()\n",
    "    print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.4\n"
     ]
    }
   ],
   "source": [
    "import parasail\n",
    "print(parasail.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6d1b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/as12267/.conda/envs/huggingface_bert/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phylo Tokenizer loaded. pad ID: 0\n",
      "Phylo Tokenizer loaded. GAP ID: 6\n",
      "vocab size: 27\n"
     ]
    }
   ],
   "source": [
    "from gLM.tokenizers import PhyloTokenizerLoader\n",
    "\n",
    "tokenizer = PhyloTokenizerLoader(\"./phylo_char_tokenizer_updated\")\n",
    "pad_id = tokenizer.pad_token_id\n",
    "gap_id = tokenizer.convert_tokens_to_ids(\"-\")\n",
    "print(\"Phylo Tokenizer loaded. pad ID:\", pad_id)\n",
    "print(\"Phylo Tokenizer loaded. GAP ID:\", gap_id)\n",
    "print(f\"vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20920a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gLM.dataset.seq_pair_iterable.SeqPairIterableDataset object at 0x155550122da0>\n"
     ]
    }
   ],
   "source": [
    "from gLM.dataset import SeqPairIterableDataset\n",
    "from gLM.tokenizers import PhyloTokenizerLoader\n",
    "from gLM.collator import PhyloCollator\n",
    "from gLM.train_utils import PhyloTrainer\n",
    "\n",
    "train_file = \"/gpfs/data/brandeslab/Data/uniref/hf_pairs_uniref90_final/train.jsonl\"\n",
    "train_ds = SeqPairIterableDataset(dataset_path=train_file,\n",
    "                                  tokenizer=tokenizer, \n",
    "                                  training_type=\"phylo_encoder_decoder\", \n",
    "                                  shuffle_buffer=100000)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb7ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = PhyloCollator(tokenizer=tokenizer, \n",
    "                         training_type =\"phylo_encoder_decoder\", \n",
    "                         max_seq_len = 8192)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7d3715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2484957/1944515737.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `PhyloTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = PhyloTrainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_start_token_id = 0\n",
      "Model parameters: 44,070,912\n",
      "Hidden size  used: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from gLM.train_utils import PhyloTrainer\n",
    "from gLM.models import ProteinT5Model\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./debug_phylo\",\n",
    "    per_device_train_batch_size=4,   \n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=2,                     \n",
    "    logging_steps=1,\n",
    "    save_steps=0,\n",
    "    report_to=\"none\")                 \n",
    "\n",
    "model = ProteinT5Model(\n",
    "            vocab_size=tokenizer.vocab_size, \n",
    "            tokenizer=tokenizer\n",
    "        ).build()\n",
    "\n",
    "model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "print(\"decoder_start_token_id =\", model.config.decoder_start_token_id)\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"Hidden size  used:\", model.config.d_model)\n",
    "\n",
    "trainer = PhyloTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2567a8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([4, 2500])\n",
      "attention_mask shape: torch.Size([4, 2500])\n",
      "attention_mask sum: tensor([ 768, 1501, 2500,  722])\n",
      "seq lengths in labels tensor([ 768, 1350, 2609,  722])\n",
      "labels shape: torch.Size([4, 2609])\n",
      "input_ids: shape=torch.Size([4, 2500]), dtype=torch.int64\n",
      "-100 counts: 0\n",
      "attention_mask: shape=torch.Size([4, 2500]), dtype=torch.int64\n",
      "-100 counts: 0\n",
      "labels: shape=torch.Size([4, 2609]), dtype=torch.int64\n",
      "-100 counts: 4987\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(f\"{k}: shape={v.shape}, dtype={v.dtype}\")\n",
    "        print(\"-100 counts:\", (v == -100).sum().item())\n",
    "    else:\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e86cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Target raw: 'MTEVFLLGRRRPEENIAGANTMGQTANEDTFDWDKYLKETGSVSAPSEYFRQSKTPPTNEFQIGMKLEARDPRNIDSMCVASVIGITGARLRLRLDGSDNKNDFWRLVDSSDIQPVGTCEQLGDLLQPPLGYRMNASSWPMFLLRILTGSELAPAVFFKEEPPRPPQNNFIVGMKIEAVDRKNPFMICPATIGAVSGDQIHITFDGWSGAFDYWCDYDSRDIFPVGWCRLTGDVLQPPGKVVEKRPRAKAVTRIWRLRNMFLENTEAAPEAAEKPGCSSFTTTSGDEDRTLEDYQAAPAGKKPRGRGFTKPLEDEVRPGRDVQVAPVEEKTKGKRVTKPWKDQATPFEDEAAAPAEKKRKSKTVTTPWTDQPRLFADQGTAPAEKKRKGKTVTTPWSDQPQLFADQGAAPAEKKHKGKTFATPWTDQPRLFADQGAAPAEKKRKGKTVTTPWTDQPRLFADQGAAPAEKKRKGKTVTTPWIDQPRLFADQGAAPAEKKRKGKTVTTSWTDQPRLFADQGAAPGEKKRKGKTVTTPWSDQPRLFADQGAAPGEKKRKGKTVTTPWSDQPRLFADQGAAPAEKKHKGKTVTTSWTDQPRLFADQGAAPAEKKRKGKTFATPWTDQPRLFADQGAAPAEKKRKGKTVTTPWIDQPRLFAVQGTAPAEKKHKGKTVTTPWSDQPRLFADQGVAPAEKKPKGKRVTKPWKDQSIPFYFGVASPAEKKPKGKRVTKRQKDQAQFLADEEAMPALFSALSVSSTEKTPPSSSEQSKSSPSGKTSSTAKGAQSSRKSPRKTSVVQKVPKTSKKAGKTKSTKDTSSTKKGITIKIVLPKKRGGKSGKKEKSIPVVSSTSSASLSSLMKSSSKTPEEPSKIVMSTVCVYINKHGDCGPYLDPQKVQQLPSHFGPGPVNVILRQIVQACVDCAIVPKDVFLLLKPDNRGGEMITAFFDGKVNTVQLPPVNSASFALRFLENFCHSLNCDKFLSSQPFRRKAHTPTDQSEPETENEELQEKKNFKRLSLKPEHSAPVSSKVPRRSGRASKASSYIAVPDPSVLKQGFCKDPSTWSVDEVIQFMKHTDPHISGPLADLFRQHEIDGKALLLLKSELLMKYMGLKLGPALKLCYYIEKLKEVKHN'\n",
      "[0] Tokenized length: 1131\n",
      "[0] Decoded: 'M T E V F L L G R R R P E E N I A G A N T M G Q T A N E D T F D W D K Y L K E T G S V S A P S E Y F R Q S K T P P T N E F Q I G M K L E A R D P R N I D S M C V A S V I G I T G A R L R L R L D G S D N K N D F W R L V D S S D I Q P V G T C E Q L G D L L Q P P L G Y R M N A S S W P M F L L R I L T G S E L A P A V F F K E E P P R P P Q N N F I V G M K I E A V D R K N P F M I C P A T I G A V S G D Q I H I T F D G W S G A F D Y W C D Y D S R D I F P V G W C R L T G D V L Q P P G K V V E K R P R A K A V T R I W R L R N M F L E N T E A A P E A A E K P G C S S F T T T S G D E D R T L E D Y Q A A P A G K K P R G R G F T K P L E D E V R P G R D V Q V A P V E E K T K G K R V T K P W K D Q A T P F E D E A A A P A E K K R K S K T V T T P W T D Q P R L F A D Q G T A P A E K K R K G K T V T T P W S D Q P Q L F A D Q G A A P A E K K H K G K T F A T P W T D Q P R L F A D Q G A A P A E K K R K G K T V T T P W T D Q P R L F A D Q G A A P A E K K R K G K T V T T P W I D Q P R L F A D Q G A A P A E K K R K G K T V T T S W T D Q P R L F A D Q G A A P G E K K R K G K T V T T P W S D Q P R L F A D Q G A A P G E K K R K G K T V T T P W S D Q P R L F A D Q G A A P A E K K H K G K T V T T S W T D Q P R L F A D Q G A A P A E K K R K G K T F A T P W T D Q P R L F A D Q G A A P A E K K R K G K T V T T P W I D Q P R L F A V Q G T A P A E K K H K G K T V T T P W S D Q P R L F A D Q G V A P A E K K P K G K R V T K P W K D Q S I P F Y F G V A S P A E K K P K G K R V T K R Q K D Q A Q F L A D E E A M P A L F S A L S V S S T E K T P P S S S E Q S K S S P S G K T S S T A K G A Q S S R K S P R K T S V V Q K V P K T S K K A G K T K S T K D T S S T K K G I T I K I V L P K K R G G K S G K K E K S I P V V S S T S S A S L S S L M K S S S K T P E E P S K I V M S T V C V Y I N K H G D C G P Y L D P Q K V Q Q L P S H F G P G P V N V I L R Q I V Q A C V D C A I V P K D V F L L L K P D N R G G E M I T A F F D G K V N T V Q L P P V N S A S F A L R F L E N F C H S L N C D K F L S S Q P F R R K A H T P T D Q S E P E T E N E E L Q E K K N F K R L S L K P E H S A P V S S K V P R R S G R A S K A S S Y I A V P D P S V L K Q G F C K D P S T W S V D E V I Q F M K H T D P H I S G P L A D L F R Q H E I D G K A L L L L K S E L L M K Y M G L K L G P A L K L C Y Y I E K L K E V K H N [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'\n",
      "[0] All pad: False\n",
      "[1] Target raw: 'MEIRILGPFELVDGERRIRIDAPKQRALLIALVLRANRVVPTEELAERIWGDDPPSSARLTLQGYVLRVRRALAGVPDVAIVTEPAGYLLRVDPERIDAHRFTRLVEEADRAAAGGDADAEAGLLHEALGLWRGEPLTDVGSDVLWRQEAGRLAEMRARAVERRIDADLRVGRHLELIGELTALVTVHPFREGFRAQLMLALYRAGRRAEALEAYRDARAVLTRELGVEPGPRLRRLEHAVLTADPALDPAPREEHSPFALPPDVPHFVGREVELARLLAPPRTDAGPSVRAVAGMAGVGKTTLVVHAAHRLADRYPDGRLHVDLGGTTGHPMDPTAALDRFLRLLGVWGGRIRGGVDDRAALLRDRLAGRRVLLVLDNAVDEAQVRPLIPGTPGCAAFITSRSPLIGLEYADLIDLDVLAPDDAIDLLAGVIGRRRILAEEDRARTLVAQCGHLPLAVRIAAARLAGRPEWTLDRLAGQLADEHRRLDRLAAGDLEVRSSIALSQDALPPAHRRPFHLIGLLDLPDVKPWMVAALLDRPVPEAEDVIDALVDARLLEPGRADAPDGPRYRCHDLVRLFAREQALGTEAPEARDAAVLRVLRALLDLAERADDALPSRRLRLPDPSPRPNRPTSAEATPADTTPADATAAGTASADATRADAMAGSAAAAGVAAGSGASDANDTGVTAGDAAAGGATAAGVAAAPLAWFEANRQVLAMAVHRAASVGAAELSWRLAASLLSFHDLRGHWDDWRRTHEVALTAARACDDRWGETAMRYGLGLLATAQDRYKDAAGQLTAAVTGSRELGAPVRAARALNALGDVHHICDRLDDAFACFQEALRIAERHDDPTGRANSLLNLGLVHRDRGDPAASLAHLDRALETFRSAGDEYGEAHVLRFLAATHYHSRTDLAAAHRHATRAIGLFRRFGDGLAEIRSLRLRAMVLTAEGRPRAGIDLLDRCLTAFREQGDLFGEASTLWALGGAAHDAGDTVRAVACLEEALNLFRRLDVPLWEARTRSRLAEVRGEGAAPVDGALPIVITGGAPPQPSSPGSC'\n",
      "[1] Tokenized length: 1053\n",
      "[1] Decoded: 'M E I R I L G P F E L V D G E R R I R I D A P K Q R A L L I A L V L R A N R V V P T E E L A E R I W G D D P P S S A R L T L Q G Y V L R V R R A L A G V P D V A I V T E P A G Y L L R V D P E R I D A H R F T R L V E E A D R A A A G G D A D A E A G L L H E A L G L W R G E P L T D V G S D V L W R Q E A G R L A E M R A R A V E R R I D A D L R V G R H L E L I G E L T A L V T V H P F R E G F R A Q L M L A L Y R A G R R A E A L E A Y R D A R A V L T R E L G V E P G P R L R R L E H A V L T A D P A L D P A P R E E H S P F A L P P D V P H F V G R E V E L A R L L A P P R T D A G P S V R A V A G M A G V G K T T L V V H A A H R L A D R Y P D G R L H V D L G G T T G H P M D P T A A L D R F L R L L G V W G G R I R G G V D D R A A L L R D R L A G R R V L L V L D N A V D E A Q V R P L I P G T P G C A A F I T S R S P L I G L E Y A D L I D L D V L A P D D A I D L L A G V I G R R R I L A E E D R A R T L V A Q C G H L P L A V R I A A A R L A G R P E W T L D R L A G Q L A D E H R R L D R L A A G D L E V R S S I A L S Q D A L P P A H R R P F H L I G L L D L P D V K P W M V A A L L D R P V P E A E D V I D A L V D A R L L E P G R A D A P D G P R Y R C H D L V R L F A R E Q A L G T E A P E A R D A A V L R V L R A L L D L A E R A D D A L P S R R L R L P D P S P R P N R P T S A E A T P A D T T P A D A T A A G T A S A D A T R A D A M A G S A A A A G V A A G S G A S D A N D T G V T A G D A A A G G A T A A G V A A A P L A W F E A N R Q V L A M A V H R A A S V G A A E L S W R L A A S L L S F H D L R G H W D D W R R T H E V A L T A A R A C D D R W G E T A M R Y G L G L L A T A Q D R Y K D A A G Q L T A A V T G S R E L G A P V R A A R A L N A L G D V H H I C D R L D D A F A C F Q E A L R I A E R H D D P T G R A N S L L N L G L V H R D R G D P A A S L A H L D R A L E T F R S A G D E Y G E A H V L R F L A A T H Y H S R T D L A A A H R H A T R A I G L F R R F G D G L A E I R S L R L R A M V L T A E G R P R A G I D L L D R C L T A F R E Q G D L F G E A S T L W A L G G A A H D A G D T V R A V A C L E E A L N L F R R L D V P L W E A R T R S R L A E V R G E G A A P V D G A L P I V I T G G A P P Q P S S P G S C [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'\n",
      "[1] All pad: False\n",
      "[2] Target raw: 'MTADSQPNDASPQRIDLAASRIPRMPFSSPMGEEVLSPTSPPQSSQTPPQVRKKGQQAVNGQSNTTLSNQRFPMERPTKGDVGRKQERQDKSRLHGLNVVTSFSNTPGFTQKAADGNGKQDQGLGLRRHGTQESTDKTGNLGLEHQRSGRPLKPSASKGRLDDLKRAASKASTLSPSDRAVMIGISVSPEDLADHDISNNASPAEPEHLGTGRYAISRRPSVTPSIIVTPAQRKPPWSDESDELIQPPRRRARAASSVYSQAQLNGVRFINSSIVPPIPPLPPDLRRPKAEFNRSSGRQLPPSSSRIVSTSTIFDEELDSDIVGRPSTGESQVGILTKQASMDSIASIATRHRSRGWWDHIVTPFWPRSPMTFKGSSPPIPTLPNARQATEAKHDRDRPTSHNVSPETAEHEGLRSGHTSWTDMSLDAEWEKRAPDLDDQPHKELLVSEPPRQLPATDTPISPVEHEGFGAASEYYEACLYDMHSPEPYFKCQNHTCLPSQVDPTEADGGQRNASARAADGDAGPREVLERLNTEPSQSLAVQQAPRNRFSAAFREAVAPESRPKPRPVSEETEIEDLDTTPDVEEAHAAPVVRAPEPVPAAQSLLPDNDRQNTREIEEPTALPPSEPTPHQPPAYSPPRQERPPRRYVAVMPPNHQPNTFEQTGLPAPPTSGAQRQMPRDTLPMAETSRDNAESQLAGTRNMRIRSYENPETSRPETTLADLYPPPRDAGRAQRAWEIREKDTQTPREHNSKVLAGFGKCFGREKKPMSKKKKWTLIALAIGLLLMVILIMVLAMTLTRKGGPGQTVPVQTAWLNITGYPPIPTGISTIVQPKPVSEISECVVPSTMWSCDLPKEEQQSFSTGAADQPNFRVEIVFQNGTNGIANASSVNRRSYGSNHGRSYGHMVNPVSAGSFVRDRLLQARNALSGNSDSPSPPPPSQEDQTFLGNTTDNNTVPFDGEYTPFFMSFLSSSKLPSRLLKRQSSSNTDNTADPFPNITNSIPPPDTNANGTAAAAVLLPYPSAQPLRLYNRGQSTEHYGFYTYFSKSVFLKSTAPIDGISNNTSSVPDDEDGGAEEDAASVRCTWAQTRFLVQIWTNKGFVASSQASDTNSTVSSNNATNLTASSANDFQAPGSFPYPVTITLDRHGGDINSKMIYCYGIDDEENPISNQKKIELEDRSFGGTLVNPTLGLFGNANVTLADGGPGGIDGGSGGCECQWKNFGWQ'\n",
      "[2] Tokenized length: 1225\n",
      "[2] Decoded: 'M T A D S Q P N D A S P Q R I D L A A S R I P R M P F S S P M G E E V L S P T S P P Q S S Q T P P Q V R K K G Q Q A V N G Q S N T T L S N Q R F P M E R P T K G D V G R K Q E R Q D K S R L H G L N V V T S F S N T P G F T Q K A A D G N G K Q D Q G L G L R R H G T Q E S T D K T G N L G L E H Q R S G R P L K P S A S K G R L D D L K R A A S K A S T L S P S D R A V M I G I S V S P E D L A D H D I S N N A S P A E P E H L G T G R Y A I S R R P S V T P S I I V T P A Q R K P P W S D E S D E L I Q P P R R R A R A A S S V Y S Q A Q L N G V R F I N S S I V P P I P P L P P D L R R P K A E F N R S S G R Q L P P S S S R I V S T S T I F D E E L D S D I V G R P S T G E S Q V G I L T K Q A S M D S I A S I A T R H R S R G W W D H I V T P F W P R S P M T F K G S S P P I P T L P N A R Q A T E A K H D R D R P T S H N V S P E T A E H E G L R S G H T S W T D M S L D A E W E K R A P D L D D Q P H K E L L V S E P P R Q L P A T D T P I S P V E H E G F G A A S E Y Y E A C L Y D M H S P E P Y F K C Q N H T C L P S Q V D P T E A D G G Q R N A S A R A A D G D A G P R E V L E R L N T E P S Q S L A V Q Q A P R N R F S A A F R E A V A P E S R P K P R P V S E E T E I E D L D T T P D V E E A H A A P V V R A P E P V P A A Q S L L P D N D R Q N T R E I E E P T A L P P S E P T P H Q P P A Y S P P R Q E R P P R R Y V A V M P P N H Q P N T F E Q T G L P A P P T S G A Q R Q M P R D T L P M A E T S R D N A E S Q L A G T R N M R I R S Y E N P E T S R P E T T L A D L Y P P P R D A G R A Q R A W E I R E K D T Q T P R E H N S K V L A G F G K C F G R E K K P M S K K K K W T L I A L A I G L L L M V I L I M V L A M T L T R K G G P G Q T V P V Q T A W L N I T G Y P P I P T G I S T I V Q P K P V S E I S E C V V P S T M W S C D L P K E E Q Q S F S T G A A D Q P N F R V E I V F Q N G T N G I A N A S S V N R R S Y G S N H G R S Y G H M V N P V S A G S F V R D R L L Q A R N A L S G N S D S P S P P P P S Q E D Q T F L G N T T D N N T V P F D G E Y T P F F M S F L S S S K L P S R L L K R Q S S S N T D N T A D P F P N I T N S I P P P D T N A N G T A A A A V L L P Y P S A Q P L R L Y N R G Q S T E H Y G F Y T Y F S K S V F L K S T A P I D G I S N N T S S V P D D E D G G A E E D A A S V R C T W A Q T R F L V Q I W T N K G F V A S S Q A S D T N S T V S S N N A T N L T A S S A N D F Q A P G S F P Y P V T I T L D R H G G D I N S K M I Y C Y G I D D E E N P I S N Q K K I E L E D R S F G G T L V N P T L G L F G N A N V T L A D G G P G G I D G G S G G C E C Q W K N F G W Q'\n",
      "[2] All pad: False\n",
      "[3] Target raw: 'MAIKQGKVRNRTRKRMTGYVWLNRLVLSFVLVFVLVSPLPAAASPQITINPPDTSEPLEAGMLHLTGSYAGVYDVQIVVNGDSLIDAHMEGITAGTWYADIDLSMYDGQVELVARGQDAVTRYNAWSSFMQIEVKHPAANVPEVRVISPAEQEVLQGQVTVQVGASGKNGISHVQVRINGGKWQDADPTASGYLLRWNTGPFSGQIISLEAKAEDVNGNVGYSQTVYVKAGISTDVGNHGSDATGTYDGIRDGNGQIEDDNRGVMEEDAVSTMKHEVRPQDRAMWIWENDSYPLILNPGSRTVLDAMSSDTDTFGHDPIRTWYLAVGKYNGIRMLEDIRAEVRDFIRWAHDRGYQVQALIAGGTIPPYFGAYERYRKQAVAEFEQILNYNLSSKERERFDGVNMDTEPYSLPDFKNAKPSVQIQYLDMLKNLMERKQASGLSLQVGAAIPRWFDTSADATDILWNGSIKPLSEHVQDTLDYISIMDYRDQADGSVGIIDQAKGEMEYANKIGKPYSVILGVETKDIADGGDPESITFHEEGRLYMEAELDKVYAAFDGNPAFGGIAIHHYESIRNLPSVWGPEAVFWQGPPDNEPPTAVTADPQANVFDYQRIDITYGPAADNRAVEEYRIYRGTEPDFAPDETHLAGISKGLSFRDMGLLPDTSYVYKVAAVDTSGNEGPPSNPVKAHTEPTSLKPMIVSKMNLGFDGSKATVTLHVADLKTGAGIAASVSGRFTFMAGKYVNGTAAAAGSFTASSEAVSASSGEIGFAARRITADGYYWAQAYDATDSSSIRWGD'\n",
      "[3] Tokenized length: 797\n",
      "[3] Decoded: 'M A I K Q G K V R N R T R K R M T G Y V W L N R L V L S F V L V F V L V S P L P A A A S P Q I T I N P P D T S E P L E A G M L H L T G S Y A G V Y D V Q I V V N G D S L I D A H M E G I T A G T W Y A D I D L S M Y D G Q V E L V A R G Q D A V T R Y N A W S S F M Q I E V K H P A A N V P E V R V I S P A E Q E V L Q G Q V T V Q V G A S G K N G I S H V Q V R I N G G K W Q D A D P T A S G Y L L R W N T G P F S G Q I I S L E A K A E D V N G N V G Y S Q T V Y V K A G I S T D V G N H G S D A T G T Y D G I R D G N G Q I E D D N R G V M E E D A V S T M K H E V R P Q D R A M W I W E N D S Y P L I L N P G S R T V L D A M S S D T D T F G H D P I R T W Y L A V G K Y N G I R M L E D I R A E V R D F I R W A H D R G Y Q V Q A L I A G G T I P P Y F G A Y E R Y R K Q A V A E F E Q I L N Y N L S S K E R E R F D G V N M D T E P Y S L P D F K N A K P S V Q I Q Y L D M L K N L M E R K Q A S G L S L Q V G A A I P R W F D T S A D A T D I L W N G S I K P L S E H V Q D T L D Y I S I M D Y R D Q A D G S V G I I D Q A K G E M E Y A N K I G K P Y S V I L G V E T K D I A D G G D P E S I T F H E E G R L Y M E A E L D K V Y A A F D G N P A F G G I A I H H Y E S I R N L P S V W G P E A V F W Q G P P D N E P P T A V T A D P Q A N V F D Y Q R I D I T Y G P A A D N R A V E E Y R I Y R G T E P D F A P D E T H L A G I S K G L S F R D M G L L P D T S Y V Y K V A A V D T S G N E G P P S N P V K A H T E P T S L K P M I V S K M N L G F D G S K A T V T L H V A D L K T G A G I A A S V S G R F T F M A G K Y V N G T A A A A G S F T A S S E A V S A S S G E I G F A A R R I T A D G Y Y W A Q A Y D A T D S S S I R W G D [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'\n",
      "[3] All pad: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=4,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    # print(\"input_ids:\", batch[\"input_ids\"].shape)\n",
    "    # print(\"attention_mask:\", batch[\"attention_mask\"].shape)\n",
    "    # print(\"labels:\", batch[\"labels\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1953877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[17, 23, 10,  ...,  0,  0,  0],\n",
       "         [17, 10, 14,  ...,  0,  0,  0],\n",
       "         [17, 23,  7,  ..., 12, 25, 10],\n",
       "         [17, 23, 12,  ...,  0,  0,  0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[  17,   23,   10,  ..., -100, -100, -100],\n",
       "         [  17,   10,   14,  ..., -100, -100, -100],\n",
       "         [  17,   23,    7,  ...,   12,   25,   20],\n",
       "         [  17,    7,   14,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1b4abbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([4, 2500])\n",
      "attention_mask shape: torch.Size([4, 2500])\n",
      "labels shape: torch.Size([4, 2609])\n",
      "Pad ID: 0\n",
      "GAP ID: 5\n",
      "PAD count in input_ids 4509\n",
      "GAP count in input_ids 0\n",
      "PAD count in labels 0\n",
      "GAP count in labels 0\n",
      "-100 count in labels 4987\n"
     ]
    }
   ],
   "source": [
    "input_ids = batch[\"input_ids\"]\n",
    "attention_mask = batch[\"attention_mask\"]\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "gap_id = tokenizer.convert_tokens_to_ids(\"[GAP]\")\n",
    "print(\"Pad ID:\", pad_id)\n",
    "print(\"GAP ID:\", gap_id)\n",
    "print(\"PAD count in input_ids\", (input_ids == pad_id).sum().item()) # comes from seq 1\n",
    "print(\"GAP count in input_ids\", (input_ids == gap_id).sum().item()) # comes from seq 1\n",
    "print(\"PAD count in labels\", (labels == pad_id).sum().item()) # should be 0 \n",
    "print(\"GAP count in labels\", (labels == gap_id).sum().item()) # comes from seq 2\n",
    "print(\"-100 count in labels\", (labels == -100).sum().item()) # comes from seq 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b6ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437\n",
      "tensor([17,  7, 14,  ...,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(len(batch[\"input_ids\"][1]))\n",
    "print(batch[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b54bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
